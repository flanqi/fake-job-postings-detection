{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "144a589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "# nlp\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "# sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8d9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, col):\n",
    "    \"\"\"Preprocess text\"\"\"\n",
    "    st = PorterStemmer()\n",
    "    stopwords_dict = stopwords.words('english') # all stopwords in English\n",
    "    data[col] = data[col].apply(lambda x: \" \".join([st.stem(i) for i in x.split() if i not in stopwords_dict]).lower().strip(string.punctuation))\n",
    "\n",
    "def create_ngrams(text, n=2):\n",
    "    \"\"\"Create n-grams given a review\"\"\"\n",
    "    unigrams = nltk.word_tokenize(text) # text should already be cleaned\n",
    "    unigrams_joined = ' '.join(unigrams)\n",
    "\n",
    "    if len(unigrams) > 1:\n",
    "        bigrams = list(map(lambda x: '_'.join(x), zip(unigrams, unigrams[1:])))\n",
    "    else:\n",
    "        bigrams = []\n",
    "\n",
    "    bigrams_joined = ' '.join(bigrams)\n",
    "\n",
    "    if n == 2:\n",
    "        return bigrams_joined\n",
    "    elif n == 1.5:\n",
    "        return unigrams_joined + ' ' + bigrams_joined\n",
    "def ohe(data, cols=['required_education', 'required_experience', 'employment_type']):\n",
    "    \"\"\"One hot encode non-textual features\"\"\"\n",
    "\n",
    "    dummies = pd.get_dummies(data[cols])\n",
    "\n",
    "    data_new = pd.concat([data, dummies], axis=1)      \n",
    "    data_new.drop(cols, inplace=True, axis=1)\n",
    "    return data_new\n",
    "def impute(data, feature='text'):\n",
    "    \"\"\"Imputation\"\"\"\n",
    "\n",
    "    # categorical features\n",
    "    unspecified_impute_features = ['employment_type', 'required_education', 'industry',\n",
    "                                   'required_experience', 'function', 'location', 'department']\n",
    "    data[unspecified_impute_features] = data[unspecified_impute_features].fillna('Unspecified')\n",
    "\n",
    "    # textual features\n",
    "    data['company_profile'] = data['company_profile'].fillna('')\n",
    "    data['description'] = data['description'].fillna('')\n",
    "    data['requirements'] = data['requirements'].fillna('')\n",
    "    data['text'] = data['company_profile'] + data['description'] + data['requirements']\n",
    "    data['text'] = data['text'].replace({'':'Missing'})\n",
    "\n",
    "    # regrouping\n",
    "    data['benefits'] = np.where(data['benefits'].isna(), 1, 0) # missing 1\n",
    "    data['required_education'] = np.where(data['required_education'].str.contains(\"Vocational\"), 'Vocational', data['required_education'])\n",
    "\n",
    "    # salary extraction and imputation\n",
    "    data[['salary_lower', 'salary_upper']] = data['salary_range'].str.split('-', 1, expand=True)\n",
    "    data['salary_lower'] = np.where(data['salary_lower'].str.isnumeric(), data['salary_lower'], np.nan)\n",
    "    data['salary_lower'] = data['salary_lower'].astype(float)\n",
    "    data['salary_lower'] = data['salary_lower'].fillna(data['salary_lower'].median())\n",
    "    data['salary_upper'] = np.where(data['salary_upper'].str.isnumeric(), data['salary_upper'], np.nan)\n",
    "    data['salary_upper'] = data['salary_upper'].astype(float)\n",
    "    data['salary_upper'] = data['salary_upper'].fillna(data['salary_upper'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c4c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (17880, 18)\n",
      "----- percentage for each class -----\n",
      "            percentage\n",
      "fraudulent            \n",
      "0             0.951566\n",
      "1             0.048378\n",
      "----- percentage for each class after upsampling -----\n",
      "            percentage\n",
      "fraudulent            \n",
      "0             0.500000\n",
      "1             0.499295\n",
      "----- preprocess finished -----\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# settings\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "# load the news dataset\n",
    "path = '../data/fake_job_postings.csv'\n",
    "df = pd.read_csv(path)\n",
    "print('Shape of dataframe is {}'.format(df.shape))\n",
    "# df = open(path, encoding='utf8').readlines()\n",
    "# df = [json.loads(x) for x in df]\n",
    "# df = pd.DataFrame(df)\n",
    "\n",
    "print('----- percentage for each class -----')\n",
    "print(pd.DataFrame(df.groupby('fraudulent').description.count()/len(df)).rename(columns={'description':'percentage'}))\n",
    "\n",
    "# concat title and content\n",
    "feature = 'text'; target = 'fraudulent'\n",
    "\n",
    "# upsampling\n",
    "neg = df[df.fraudulent == 0]\n",
    "pos = df[df.fraudulent == 1].sample(n=len(neg), replace=True, random_state=42)\n",
    "df = pd.concat([neg, pos])\n",
    "df = df.sample(frac=1).reset_index(drop=True) # shuffle rows\n",
    "print('----- percentage for each class after upsampling -----')\n",
    "print(pd.DataFrame(df.groupby('fraudulent').description.count()/len(df)).rename(columns={'description':'percentage'}))\n",
    "\n",
    "# print(df.head())\n",
    "# print(df.tail())\n",
    "\n",
    "# data cleaning, remove stopwords and perform stemming\n",
    "impute(df, feature) # this will automatically create text col\n",
    "preprocess(df, feature)\n",
    "print('----- preprocess finished -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe25fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ohe(df) # one hot encoding \n",
    "add_features = ['benefits', 'has_company_logo', 'has_questions', 'telecommuting', # binary\n",
    "                        'salary_lower', 'salary_upper', # numerical\n",
    "                        'required_education.*', 'required_experience.*', 'employment_type.*'] # categorical\n",
    "add_features = df.filter(regex='|'.join(add_features)).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cc3b376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_multimodal_wrapper(data, add_features, feature='text', target='fraudulent', k=1000, max_iter=500, min_df=3, loss='hinge', alpha=1e-4, path='svm_mult.pickle'):\n",
    "    \"\"\"SVM multimodal pipeline\"\"\"\n",
    "    X = data[[feature]+add_features]\n",
    "    y = data[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    vec = TfidfVectorizer(min_df=min_df, stop_words='english', sublinear_tf=True, ngram_range=(1,2))\n",
    "    score_func = SelectKBest(chi2, k=k)\n",
    "    mod = SGDClassifier(loss=loss, penalty='l2', alpha=alpha, random_state=42, max_iter=max_iter, tol=None)\n",
    "    \n",
    "    # save tf-idf to dataframe\n",
    "    X_train1 = vec.fit_transform(X_train[feature])\n",
    "    X_train1 = score_func.fit_transform(X_train1, y_train)\n",
    "    train_data = pd.DataFrame(X_train1.toarray(), columns=sel.get_support(indices=True))\n",
    "    train_data = pd.concat([y_train, X_train[add_features], train_data], axis=1) # append tf-idf features to meda features and target\n",
    "    print(train_data.head())\n",
    "    model = mod.fit(train_data[1:], train_data[target]) # model training\n",
    "\n",
    "    # tranform test data for prediction and evaluation\n",
    "    X_test1 = vec.transform(X_test1[feature])\n",
    "    X_test1 = score_func.transform(X_test1)\n",
    "    test_data = pd.DataFrame(X_test1.toarray(), columns=sel.get_support(indices=True))\n",
    "    test_data = pd.concat([y_test, X_test[add_features], test_data], axis=1) # append tf-idf features to meda features and target\n",
    "\n",
    "    print(classification_report(np.array(y_test), model.predict(test_data)))\n",
    "    print(confusion_matrix(np.array(y_test), model.predict(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a76e3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_multimodal_wrapper(data=df, add_features=add_features, k=1000, loss='hinge', path='svm_mult0.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "475c5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df; k=1000; max_iter=500; min_df=3; loss='hinge'; alpha=1e-4; feature='text'\n",
    "X = data[[feature]+add_features]\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "vec = TfidfVectorizer(min_df=min_df, stop_words='english', sublinear_tf=True, ngram_range=(1,2))\n",
    "score_func = SelectKBest(chi2, k=k)\n",
    "# mod = SGDClassifier(loss=loss, penalty='l2', alpha=alpha, random_state=42, max_iter=max_iter, tol=None)\n",
    "mod = LogisticRegression(random_state=42, max_iter=max_iter, C=1, solver='saga', l1_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd902ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tf-idf to dataframe\n",
    "X_train1 = vec.fit_transform(X_train[feature])\n",
    "X_train1 = score_func.fit_transform(X_train1, y_train)\n",
    "train_data = pd.DataFrame(X_train1.toarray(), columns=score_func.get_support(indices=True))\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "99f00ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index(); y_train.drop(columns={'index'}, inplace=True)\n",
    "X_train = X_train.reset_index(); X_train.drop(columns={'index'}, inplace=True)\n",
    "train_data = train_data.reset_index(); train_data.drop(columns={'index'}, inplace=True)\n",
    "train_data = pd.concat([y_train, X_train[add_features], train_data], axis=1) # append tf-idf features to meda features and target\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ffcab978",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mod.fit(train_data.iloc[:,1:], train_data[target]) # model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d800a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform test data for prediction and evaluation\n",
    "X_test1 = vec.transform(X_test[feature])\n",
    "X_test1 = score_func.transform(X_test1)\n",
    "test_data = pd.DataFrame(X_test1.toarray(), columns=score_func.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "713feceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reset_index(); y_test.drop(columns={'index'}, inplace=True)\n",
    "X_test = X_test.reset_index(); X_test.drop(columns={'index'}, inplace=True)\n",
    "test_data = test_data.reset_index(); test_data.drop(columns={'index'}, inplace=True)\n",
    "test_data = pd.concat([y_test, X_test[add_features], test_data], axis=1) # append tf-idf features to meda features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a1971953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.98      0.67      5074\n",
      "           1       0.71      0.04      0.08      5135\n",
      "\n",
      "    accuracy                           0.51     10209\n",
      "   macro avg       0.61      0.51      0.37     10209\n",
      "weighted avg       0.61      0.51      0.37     10209\n",
      "\n",
      "[[4989   85]\n",
      " [4929  206]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.array(y_test), model.predict(test_data.iloc[:,1:])))\n",
    "print(confusion_matrix(np.array(y_test), model.predict(test_data.iloc[:,1:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
